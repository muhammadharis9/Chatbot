# Chatbot â€“ RAG Chatbot over the â€œAgentic AIâ€ Book (LangChain + Pinecone + OpenRouter)

This repo contains a simple end-to-end **Retrieval-Augmented Generation (RAG)** chatbot.

By default, it is designed to chat over the **â€œAgentic AIâ€ book** (or any similar PDF you put in the `dataset/` folder). The bot **does not invent knowledge on its own** â€“ instead, it retrieves relevant passages from the book and uses them as context for the answer.

> âš ï¸ This project is for learning and experimentation only.  
> It must **not** be used for real medical or clinical decision-making.

---

## âœ¨ What This Project Actually Is

You can think of this project as:

> ğŸ§  **â€œAsk-me-anything about the Agentic AI bookâ€** â€“ powered by RAG.

Concretely:

1. You drop the **Agentic AI book PDF** into `dataset/`.
2. The project:
   - Loads the PDF
   - Splits it into small overlapping chunks
   - Embeds those chunks using a sentence-transformer model
   - Stores the vectors in a **Pinecone** index
3. When a user asks a question in the **Flask web UI**:
   - The system retrieves the most relevant chunks from the book
   - Injects them into a carefully designed system prompt
   - Sends everything to an LLM via **OpenRouter**
   - Returns an answer that is grounded in the *actual text* of the book

So you end up with a small, focused **Agentic AI RAG chatbot**.

You can easily swap the book for any other domain PDF (support docs, internal guides, research papers, etc.) and reuse the same pipeline.

---

## ğŸ§° Tech Stack

- **Language:** Python 3.11+
- **Frameworks:** Flask, LangChain
- **Vector Store:** Pinecone
- **Embeddings:** `sentence-transformers/all-MiniLM-L6-v2` (via `langchain-huggingface`)
- **LLM Access:** `langchain-openai` with OpenRouter as the backend
- **Env / Deps:** [`uv`](https://github.com/astral-sh/uv), `pyproject.toml`
- **Frontend:** HTML template (`templates/chat.html`) + CSS (`static/style.css`)
- **Notebook Exploration:** Jupyter notebook in `research/`

---

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ app.py                # Main Flask app and RAG pipeline
â”œâ”€â”€ main.py               # Simple CLI entry point (prints â€œHello from chatbot!â€)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ helper.py         # PDF loading, metadata cleanup, chunking, embeddings
â”‚   â””â”€â”€ prompt.py         # System prompt for the chatbot
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ chat.html         # Chat UI rendered by Flask
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css         # Styles for the chat interface
â”œâ”€â”€ dataset/              # Folder where you place your PDFs (e.g. Agentic AI book)
â”œâ”€â”€ research/
â”‚   â””â”€â”€ notebook_test.ipynb  # Notebook showing the RAG + Pinecone pipeline
â”œâ”€â”€ pyproject.toml        # Project metadata & dependencies (managed by uv)
â”œâ”€â”€ uv.lock               # Locked dependency versions
â”œâ”€â”€ .python-version       # Python version used by the project
â”œâ”€â”€ .env                  # Environment variables (API keys, etc.) â€“ do NOT commit real keys
â”œâ”€â”€ template.sh           # Shell script placeholder (can be used for automation)
â”œâ”€â”€ LICENSE               # Apache-2.0 license
â””â”€â”€ README.md             # This file
